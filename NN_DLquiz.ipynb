{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess the dataset (e.g., MNIST)\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Reshape the data to match the expected input shape of the model\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "# Add random noise to the training data\n",
        "noise_factor = 0.2\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip the values to ensure they are in the range [0, 1]\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=input_shape)\n",
        "conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "\n",
        "# Decoder\n",
        "conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "up1 = UpSampling2D((2, 2))(conv3)\n",
        "conv4 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)\n",
        "up2 = UpSampling2D((2, 2))(conv4)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)\n",
        "\n",
        "# Define the autoencoder model\n",
        "autoencoder = Model(inputs, decoded)\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Compile the model\n",
        "autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "autoencoder.fit(x_train_noisy, x_train,\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "decoded_imgs = autoencoder.predict(x_test_noisy)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "mse = K.mean(K.square(x_test - decoded_imgs))\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Visualize the reconstructed images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Define the autoencoder model\n",
        "def create_autoencoder(learning_rate=0.001, loss='binary_crossentropy'):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    pool1 = MaxPooling2D((2, 2), padding='same')(conv1)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
        "    pool2 = MaxPooling2D((2, 2), padding='same')(conv2)\n",
        "    encoded = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
        "\n",
        "    conv3 = Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
        "    up1 = UpSampling2D((2, 2))(conv3)\n",
        "    conv4 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1)\n",
        "    up2 = UpSampling2D((2, 2))(conv4)\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2)\n",
        "\n",
        "    autoencoder = Model(inputs, decoded)\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    autoencoder.compile(optimizer=optimizer, loss=loss)\n",
        "    return autoencoder\n",
        "\n",
        "# Create a KerasClassifier\n",
        "autoencoder_clf = KerasClassifier(build_fn=create_autoencoder, verbose=0)\n",
        "\n",
        "# Define the hyperparameters to tune\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "losses = ['binary_crossentropy', 'mean_squared_error']\n",
        "param_grid = dict(learning_rate=learning_rates, loss=losses)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(estimator=autoencoder_clf, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_search = grid_search.fit(x_train_noisy, x_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_.model\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "decoded_imgs = best_model.predict(x_test_noisy)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "mse = K.mean(K.square(x_test - decoded_imgs))\n",
        "print('Mean Squared Error:', mse)\n",
        "\n",
        "# Visualize the reconstructed images\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    # Display original\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display reconstruction\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNXnh0-3AyzN",
        "outputId": "39201a20-2e21-41c5-857f-2600900d251f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            " 58/469 [==>...........................] - ETA: 3:35 - loss: 0.2794"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Load the IMDb dataset\n",
        "max_features = 10000\n",
        "maxlen = 200\n",
        "(X_train_txt, y_train_txt), (X_test_txt, y_test_txt) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# Preprocess the data\n",
        "X_train_txt = pad_sequences(X_train_txt, maxlen=maxlen)\n",
        "X_test_txt = pad_sequences(X_test_txt, maxlen=maxlen)\n",
        "\n",
        "# Build RNN model\n",
        "def create_rnn_model(optimizer='adam'):\n",
        "    model = Sequential([\n",
        "        Embedding(max_features, 32),\n",
        "        SimpleRNN(32),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Perform grid search for RNN model hyperparameters\n",
        "rnn_model = create_rnn_model()\n",
        "rnn_param_grid = {'optimizer': ['adam', 'rmsprop'],\n",
        "                  'batch_size': [32, 64],\n",
        "                  'learning_rate': [0.01, 0.001]}\n",
        "rnn_grid = GridSearchCV(estimator=rnn_model, param_grid=rnn_param_grid, cv=3)\n",
        "rnn_grid_result = rnn_grid.fit(X_train_txt, y_train_txt)\n",
        "\n",
        "# Print RNN grid search results\n",
        "print(\"Best RNN Model: %f using %s\" % (rnn_grid_result.best_score_, rnn_grid_result.best_params_))\n",
        "\n",
        "# Train the final RNN model with best hyperparameters\n",
        "best_rnn_params = rnn_grid_result.best_params_\n",
        "final_rnn_model = create_rnn_model(optimizer=best_rnn_params['optimizer'])\n",
        "rnn_history = final_rnn_model.fit(X_train_txt, y_train_txt, epochs=5, batch_size=best_rnn_params['batch_size'], validation_split=0.2)\n",
        "\n",
        "# Evaluate RNN model on test set\n",
        "rnn_test_loss, rnn_test_acc = final_rnn_model.evaluate(X_test_txt, y_test_txt)\n",
        "print('RNN Test Accuracy:', rnn_test_acc)\n",
        "\n",
        "# Plot RNN model training history\n",
        "plt.plot(rnn_history.history['accuracy'], label='accuracy')\n",
        "plt.plot(rnn_history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "SOKQLCgMA5kF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}